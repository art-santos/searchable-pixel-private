{"version":3,"sources":["../src/index.ts","../src/constants.ts","../src/detector.ts","../src/tracker.ts"],"sourcesContent":["// Main exports\nexport { detectAICrawler, extractRequestMetadata } from './detector'\nexport { CrawlerTracker } from './tracker'\nexport { AI_CRAWLERS } from './constants'\nexport type { CrawlerDetectionResult, UnknownCrawlerInfo } from './detector'\nexport type { CrawlerEvent, TrackerConfig, PingResponse } from './tracker'\nexport type { CrawlerInfo } from './constants'\n\n// For custom Node.js/Express servers\nimport { IncomingMessage, ServerResponse } from 'http'\nimport { detectAICrawler, extractRequestMetadata } from './detector'\nimport { CrawlerTracker, TrackerConfig, PingResponse } from './tracker'\n\n/**\n * Ping the Split Analytics API to verify connection and API key validity\n */\nexport async function ping(config: TrackerConfig): Promise<PingResponse> {\n  const tracker = new CrawlerTracker(config)\n  const result = await tracker.ping()\n  tracker.destroy()\n  return result\n}\n\n/**\n * Express/Node.js middleware for crawler tracking\n */\nexport function createNodeMiddleware(config: TrackerConfig) {\n  const tracker = new CrawlerTracker(config)\n\n  return function middleware(\n    req: IncomingMessage & { url?: string },\n    res: ServerResponse,\n    next: () => void\n  ) {\n    const startTime = Date.now()\n    \n    try {\n      // Get user agent\n      const userAgent = req.headers['user-agent'] || ''\n      const detection = detectAICrawler(userAgent)\n\n      if (!detection.isAICrawler || !detection.crawler) {\n        return next()\n      }\n\n      // Store crawler info to use in the response handler\n      const crawlerInfo = detection.crawler\n\n      // Hook into response to get status code\n      const originalEnd = res.end\n      res.end = function(this: ServerResponse, ...args: Parameters<typeof originalEnd>): ReturnType<typeof originalEnd> {\n        const responseTimeMs = Date.now() - startTime\n        \n        // Track the event\n        const fullUrl = `${req.headers.host}${req.url}`\n        const event = tracker.createEvent(\n          crawlerInfo,\n          {\n            url: fullUrl,\n            headers: req.headers as any,\n            statusCode: res.statusCode,\n            responseTimeMs\n          },\n          extractRequestMetadata({ headers: req.headers as any })\n        )\n\n        tracker.track(event).catch(error => {\n          if (config.debug) {\n            console.error('[Split Analytics] Failed to track event:', error)\n          }\n        })\n\n        // Call original end\n        return originalEnd.apply(this, args)\n      } as typeof originalEnd\n\n      next()\n    } catch (error) {\n      if (config.debug) {\n        console.error('[Split Analytics] Middleware error:', error)\n      }\n      next()\n    }\n  }\n}\n\n/**\n * Simple tracking function for custom implementations\n */\nexport async function trackCrawler(\n  config: TrackerConfig,\n  request: {\n    url: string\n    userAgent: string\n    headers?: Record<string, string | string[] | undefined>\n    statusCode?: number\n    responseTimeMs?: number\n  }\n): Promise<boolean> {\n  const detection = detectAICrawler(request.userAgent)\n  \n  if (!detection.isAICrawler || !detection.crawler) {\n    return false\n  }\n\n  const tracker = new CrawlerTracker(config)\n  \n  const event = tracker.createEvent(\n    detection.crawler,\n    {\n      url: request.url,\n      headers: { \n        'user-agent': request.userAgent,\n        ...(request.headers || {})\n      },\n      statusCode: request.statusCode,\n      responseTimeMs: request.responseTimeMs\n    },\n    request.headers ? extractRequestMetadata({ headers: request.headers }) : undefined\n  )\n\n  await tracker.track(event)\n  \n  // Clean up single-use tracker\n  tracker.destroy()\n  \n  return true\n} ","// Known AI crawler user agents\nexport const AI_CRAWLERS = {\n  // OpenAI (3 main crawlers)\n  'GPTBot': { company: 'OpenAI', bot: 'GPTBot', category: 'ai-training' },\n  'ChatGPT-User': { company: 'OpenAI', bot: 'ChatGPT-User', category: 'ai-assistant' },\n  'OAI-SearchBot': { company: 'OpenAI', bot: 'OAI-SearchBot', category: 'ai-search' },\n  \n  // Anthropic\n  'Claude-Web': { company: 'Anthropic', bot: 'Claude-Web', category: 'ai-assistant' },\n  'ClaudeBot': { company: 'Anthropic', bot: 'ClaudeBot', category: 'ai-training' },\n  'anthropic-ai': { company: 'Anthropic', bot: 'anthropic-ai', category: 'ai-training' },\n  \n  // Google/Alphabet\n  'Google-Extended': { company: 'Google', bot: 'Google-Extended', category: 'ai-training' },\n  'Googlebot': { company: 'Google', bot: 'Googlebot', category: 'search-ai' },\n  'Googlebot-Image': { company: 'Google', bot: 'Googlebot-Image', category: 'search-ai' },\n  'Googlebot-News': { company: 'Google', bot: 'Googlebot-News', category: 'search-ai' },\n  'Google-InspectionTool': { company: 'Google', bot: 'Google-InspectionTool', category: 'search-ai' },\n  \n  // Microsoft\n  'Bingbot': { company: 'Microsoft', bot: 'Bingbot', category: 'search-ai' },\n  'msnbot': { company: 'Microsoft', bot: 'msnbot', category: 'search-ai' },\n  'BingPreview': { company: 'Microsoft', bot: 'BingPreview', category: 'search-ai' },\n  \n  // Perplexity\n  'PerplexityBot': { company: 'Perplexity', bot: 'PerplexityBot', category: 'ai-search' },\n  \n  // Meta/Facebook\n  'FacebookBot': { company: 'Meta', bot: 'FacebookBot', category: 'social-ai' },\n  'facebookexternalhit': { company: 'Meta', bot: 'facebookexternalhit', category: 'social-ai' },\n  'Meta-ExternalAgent': { company: 'Meta', bot: 'Meta-ExternalAgent', category: 'ai-training' },\n  \n  // Other AI Search Engines\n  'YouBot': { company: 'You.com', bot: 'YouBot', category: 'ai-search' },\n  'Neeva': { company: 'Neeva', bot: 'Neeva', category: 'ai-search' },\n  'Phind': { company: 'Phind', bot: 'Phind', category: 'ai-search' },\n  \n  // Chinese AI Companies\n  'Bytespider': { company: 'ByteDance', bot: 'Bytespider', category: 'ai-training' },\n  'Baiduspider': { company: 'Baidu', bot: 'Baiduspider', category: 'search-ai' },\n  'Sogou': { company: 'Sogou', bot: 'Sogou', category: 'search-ai' },\n  \n  // E-commerce & Enterprise\n  'Amazonbot': { company: 'Amazon', bot: 'Amazonbot', category: 'ai-assistant' },\n  'LinkedInBot': { company: 'LinkedIn', bot: 'LinkedInBot', category: 'social-ai' },\n  'Twitterbot': { company: 'Twitter', bot: 'Twitterbot', category: 'social-ai' },\n  \n  // Apple\n  'Applebot': { company: 'Apple', bot: 'Applebot', category: 'search-ai' },\n  'Applebot-Extended': { company: 'Apple', bot: 'Applebot-Extended', category: 'ai-training' },\n  \n  // Data Extraction & Analysis\n  'Diffbot': { company: 'Diffbot', bot: 'Diffbot', category: 'ai-extraction' },\n  'DataForSeoBot': { company: 'DataForSEO', bot: 'DataForSeoBot', category: 'ai-extraction' },\n  'SemrushBot': { company: 'Semrush', bot: 'SemrushBot', category: 'ai-extraction' },\n  'AhrefsBot': { company: 'Ahrefs', bot: 'AhrefsBot', category: 'ai-extraction' },\n  \n  // Common Crawl & Research\n  'CCBot': { company: 'Common Crawl', bot: 'CCBot', category: 'ai-training' },\n  'ia_archiver': { company: 'Internet Archive', bot: 'ia_archiver', category: 'archival' },\n  \n  // Other Notable AI Crawlers\n  'PetalBot': { company: 'Petal Search', bot: 'PetalBot', category: 'search-ai' },\n  'SeznamBot': { company: 'Seznam', bot: 'SeznamBot', category: 'search-ai' },\n  'Yandex': { company: 'Yandex', bot: 'YandexBot', category: 'search-ai' },\n  'DuckDuckBot': { company: 'DuckDuckGo', bot: 'DuckDuckBot', category: 'search-ai' },\n  'Qwantify': { company: 'Qwant', bot: 'Qwantify', category: 'search-ai' },\n} as const\n\nexport type CrawlerInfo = typeof AI_CRAWLERS[keyof typeof AI_CRAWLERS]\n\n// Default API endpoint\nexport const DEFAULT_API_ENDPOINT = 'https://split.dev/api/crawler-events'\n\n// Batch settings\nexport const BATCH_SIZE = 10\nexport const BATCH_INTERVAL_MS = 5000 // 5 seconds\nexport const MAX_RETRY_ATTEMPTS = 3\nexport const RETRY_DELAY_MS = 1000 ","import { AI_CRAWLERS, CrawlerInfo } from './constants'\n\n// Type for unknown crawlers\nexport interface UnknownCrawlerInfo {\n  company: string\n  bot: string\n  category: string\n}\n\nexport interface CrawlerDetectionResult {\n  isAICrawler: boolean\n  crawler?: CrawlerInfo | UnknownCrawlerInfo\n  userAgent: string\n}\n\n/**\n * Detects if a user agent string belongs to an AI crawler\n */\nexport function detectAICrawler(userAgent: string | null): CrawlerDetectionResult {\n  if (!userAgent) {\n    return {\n      isAICrawler: false,\n      userAgent: ''\n    }\n  }\n\n  // Check each known crawler pattern\n  for (const [pattern, info] of Object.entries(AI_CRAWLERS)) {\n    if (userAgent.includes(pattern)) {\n      return {\n        isAICrawler: true,\n        crawler: info,\n        userAgent\n      }\n    }\n  }\n\n  // Additional checks for less specific patterns\n  const lowerUA = userAgent.toLowerCase()\n  \n  // Check for bot patterns that might indicate AI crawlers\n  const aiPatterns = [\n    'ai-crawler',\n    'ai-bot',\n    'llm-crawler',\n    'training-bot',\n    'ml-bot'\n  ]\n  \n  for (const pattern of aiPatterns) {\n    if (lowerUA.includes(pattern)) {\n      return {\n        isAICrawler: true,\n        crawler: {\n          company: 'Unknown',\n          bot: pattern,\n          category: 'unknown'\n        } as UnknownCrawlerInfo,\n        userAgent\n      }\n    }\n  }\n\n  return {\n    isAICrawler: false,\n    userAgent\n  }\n}\n\n/**\n * Extract additional metadata from the request\n */\nexport function extractRequestMetadata(request: {\n  headers: Record<string, string | string[] | undefined>\n  url?: string\n  method?: string\n}) {\n  const metadata: Record<string, any> = {}\n\n  // Extract referer\n  const referer = request.headers['referer'] || request.headers['referrer']\n  if (referer) {\n    metadata.referer = Array.isArray(referer) ? referer[0] : referer\n  }\n\n  // Extract accept headers to understand what the crawler wants\n  const accept = request.headers['accept']\n  if (accept) {\n    metadata.accept = Array.isArray(accept) ? accept.join(', ') : accept\n  }\n\n  // Extract encoding\n  const encoding = request.headers['accept-encoding']\n  if (encoding) {\n    metadata.acceptEncoding = Array.isArray(encoding) ? encoding.join(', ') : encoding\n  }\n\n  // Extract language preferences\n  const language = request.headers['accept-language']\n  if (language) {\n    metadata.acceptLanguage = Array.isArray(language) ? language[0] : language\n  }\n\n  return metadata\n} ","import { CrawlerInfo } from './constants'\nimport { UnknownCrawlerInfo } from './detector'\nimport { \n  DEFAULT_API_ENDPOINT, \n  BATCH_SIZE, \n  BATCH_INTERVAL_MS,\n  MAX_RETRY_ATTEMPTS,\n  RETRY_DELAY_MS \n} from './constants'\n\nexport interface CrawlerEvent {\n  timestamp: string\n  domain: string\n  path: string\n  crawlerName: string\n  crawlerCompany: string\n  crawlerCategory: string\n  userAgent: string\n  statusCode?: number\n  responseTimeMs?: number\n  country?: string\n  metadata?: Record<string, any>\n}\n\nexport interface TrackerConfig {\n  apiKey: string\n  apiEndpoint?: string\n  batchSize?: number\n  batchIntervalMs?: number\n  debug?: boolean\n  onError?: (error: Error) => void\n}\n\nexport interface PingResponse {\n  status: 'ok' | 'error'\n  connection?: {\n    authenticated: boolean\n    keyName: string\n    workspace: string\n    domain: string | null\n  }\n  message?: string\n  timestamp?: string\n}\n\nexport class CrawlerTracker {\n  private config: Required<TrackerConfig>\n  private eventQueue: CrawlerEvent[] = []\n  private batchTimer: NodeJS.Timeout | null = null\n  private isSending = false\n\n  constructor(config: TrackerConfig) {\n    this.config = {\n      apiKey: config.apiKey,\n      apiEndpoint: config.apiEndpoint || DEFAULT_API_ENDPOINT,\n      batchSize: config.batchSize || BATCH_SIZE,\n      batchIntervalMs: config.batchIntervalMs || BATCH_INTERVAL_MS,\n      debug: config.debug || false,\n      onError: config.onError || ((error) => console.error('[Split Analytics]', error))\n    }\n  }\n\n  /**\n   * Track a crawler visit\n   */\n  async track(event: Omit<CrawlerEvent, 'timestamp'>) {\n    const fullEvent: CrawlerEvent = {\n      ...event,\n      timestamp: new Date().toISOString()\n    }\n\n    this.eventQueue.push(fullEvent)\n\n    if (this.config.debug) {\n      console.log('[Split Analytics] Event queued:', fullEvent)\n    }\n\n    // Check if we should send immediately\n    if (this.eventQueue.length >= this.config.batchSize) {\n      await this.flush()\n    } else {\n      // Schedule batch send\n      this.scheduleBatch()\n    }\n  }\n\n  /**\n   * Create a tracking event from request data\n   */\n  createEvent(\n    crawler: CrawlerInfo | UnknownCrawlerInfo,\n    request: {\n      url: string\n      headers: Record<string, string | string[] | undefined>\n      statusCode?: number\n      responseTimeMs?: number\n    },\n    metadata?: Record<string, any>\n  ): Omit<CrawlerEvent, 'timestamp'> {\n    const url = new URL(request.url)\n    \n    return {\n      domain: url.hostname,\n      path: url.pathname,\n      crawlerName: crawler.bot,\n      crawlerCompany: crawler.company,\n      crawlerCategory: crawler.category,\n      userAgent: this.getHeaderValue(request.headers['user-agent']) || '',\n      statusCode: request.statusCode,\n      responseTimeMs: request.responseTimeMs,\n      metadata\n    }\n  }\n\n  /**\n   * Schedule a batch send\n   */\n  private scheduleBatch() {\n    if (this.batchTimer) return\n\n    this.batchTimer = setTimeout(async () => {\n      this.batchTimer = null\n      if (this.eventQueue.length > 0) {\n        await this.flush()\n      }\n    }, this.config.batchIntervalMs)\n  }\n\n  /**\n   * Send all queued events\n   */\n  async flush() {\n    if (this.isSending || this.eventQueue.length === 0) return\n\n    this.isSending = true\n    const events = [...this.eventQueue]\n    this.eventQueue = []\n\n    try {\n      await this.sendBatch(events)\n    } catch (error) {\n      // Put events back in queue for retry\n      this.eventQueue.unshift(...events)\n      this.config.onError(error as Error)\n    } finally {\n      this.isSending = false\n    }\n  }\n\n  /**\n   * Send a batch of events with retry logic\n   */\n  private async sendBatch(events: CrawlerEvent[], attempt = 1): Promise<void> {\n    try {\n      const response = await fetch(this.config.apiEndpoint, {\n        method: 'POST',\n        headers: {\n          'Content-Type': 'application/json',\n          'Authorization': `Bearer ${this.config.apiKey}`,\n          'X-Split-Version': '0.1.0'\n        },\n        body: JSON.stringify({ events })\n      })\n\n      if (!response.ok) {\n        throw new Error(`API error: ${response.status} ${response.statusText}`)\n      }\n\n      if (this.config.debug) {\n        console.log(`[Split Analytics] Sent ${events.length} events`)\n      }\n    } catch (error) {\n      if (attempt < MAX_RETRY_ATTEMPTS) {\n        // Exponential backoff\n        const delay = RETRY_DELAY_MS * Math.pow(2, attempt - 1)\n        if (this.config.debug) {\n          console.log(`[Split Analytics] Retry attempt ${attempt} after ${delay}ms`)\n        }\n        await new Promise(resolve => setTimeout(resolve, delay))\n        return this.sendBatch(events, attempt + 1)\n      }\n      throw error\n    }\n  }\n\n  /**\n   * Helper to get header value\n   */\n  private getHeaderValue(header: string | string[] | undefined): string | undefined {\n    if (!header) return undefined\n    return Array.isArray(header) ? header[0] : header\n  }\n\n  /**\n   * Cleanup resources\n   */\n  destroy() {\n    if (this.batchTimer) {\n      clearTimeout(this.batchTimer)\n      this.batchTimer = null\n    }\n    // Try to send remaining events\n    this.flush().catch(() => {})\n  }\n\n  /**\n   * Ping the API to verify connection and API key validity\n   */\n  async ping(): Promise<PingResponse> {\n    try {\n      const pingEndpoint = this.config.apiEndpoint.replace(/\\/events$/, '/ping')\n      \n      const response = await fetch(pingEndpoint, {\n        method: 'GET',\n        headers: {\n          'Authorization': `Bearer ${this.config.apiKey}`,\n          'X-Split-Version': '0.1.0'\n        }\n      })\n\n      const data = await response.json() as PingResponse\n\n      if (!response.ok) {\n        return {\n          status: 'error',\n          message: data.message || `API error: ${response.status}`\n        }\n      }\n\n      if (this.config.debug) {\n        console.log('[Split Analytics] Ping successful:', data)\n      }\n\n      return data\n    } catch (error) {\n      if (this.config.debug) {\n        console.error('[Split Analytics] Ping failed:', error)\n      }\n      \n      return {\n        status: 'error',\n        message: error instanceof Error ? error.message : 'Connection failed'\n      }\n    }\n  }\n} "],"mappings":"yaAAA,IAAAA,EAAA,GAAAC,EAAAD,EAAA,iBAAAE,EAAA,mBAAAC,EAAA,yBAAAC,EAAA,oBAAAC,EAAA,2BAAAC,EAAA,SAAAC,EAAA,iBAAAC,IAAA,eAAAC,EAAAT,GCCO,IAAMU,EAAc,CAEzB,OAAU,CAAE,QAAS,SAAU,IAAK,SAAU,SAAU,aAAc,EACtE,eAAgB,CAAE,QAAS,SAAU,IAAK,eAAgB,SAAU,cAAe,EACnF,gBAAiB,CAAE,QAAS,SAAU,IAAK,gBAAiB,SAAU,WAAY,EAGlF,aAAc,CAAE,QAAS,YAAa,IAAK,aAAc,SAAU,cAAe,EAClF,UAAa,CAAE,QAAS,YAAa,IAAK,YAAa,SAAU,aAAc,EAC/E,eAAgB,CAAE,QAAS,YAAa,IAAK,eAAgB,SAAU,aAAc,EAGrF,kBAAmB,CAAE,QAAS,SAAU,IAAK,kBAAmB,SAAU,aAAc,EACxF,UAAa,CAAE,QAAS,SAAU,IAAK,YAAa,SAAU,WAAY,EAC1E,kBAAmB,CAAE,QAAS,SAAU,IAAK,kBAAmB,SAAU,WAAY,EACtF,iBAAkB,CAAE,QAAS,SAAU,IAAK,iBAAkB,SAAU,WAAY,EACpF,wBAAyB,CAAE,QAAS,SAAU,IAAK,wBAAyB,SAAU,WAAY,EAGlG,QAAW,CAAE,QAAS,YAAa,IAAK,UAAW,SAAU,WAAY,EACzE,OAAU,CAAE,QAAS,YAAa,IAAK,SAAU,SAAU,WAAY,EACvE,YAAe,CAAE,QAAS,YAAa,IAAK,cAAe,SAAU,WAAY,EAGjF,cAAiB,CAAE,QAAS,aAAc,IAAK,gBAAiB,SAAU,WAAY,EAGtF,YAAe,CAAE,QAAS,OAAQ,IAAK,cAAe,SAAU,WAAY,EAC5E,oBAAuB,CAAE,QAAS,OAAQ,IAAK,sBAAuB,SAAU,WAAY,EAC5F,qBAAsB,CAAE,QAAS,OAAQ,IAAK,qBAAsB,SAAU,aAAc,EAG5F,OAAU,CAAE,QAAS,UAAW,IAAK,SAAU,SAAU,WAAY,EACrE,MAAS,CAAE,QAAS,QAAS,IAAK,QAAS,SAAU,WAAY,EACjE,MAAS,CAAE,QAAS,QAAS,IAAK,QAAS,SAAU,WAAY,EAGjE,WAAc,CAAE,QAAS,YAAa,IAAK,aAAc,SAAU,aAAc,EACjF,YAAe,CAAE,QAAS,QAAS,IAAK,cAAe,SAAU,WAAY,EAC7E,MAAS,CAAE,QAAS,QAAS,IAAK,QAAS,SAAU,WAAY,EAGjE,UAAa,CAAE,QAAS,SAAU,IAAK,YAAa,SAAU,cAAe,EAC7E,YAAe,CAAE,QAAS,WAAY,IAAK,cAAe,SAAU,WAAY,EAChF,WAAc,CAAE,QAAS,UAAW,IAAK,aAAc,SAAU,WAAY,EAG7E,SAAY,CAAE,QAAS,QAAS,IAAK,WAAY,SAAU,WAAY,EACvE,oBAAqB,CAAE,QAAS,QAAS,IAAK,oBAAqB,SAAU,aAAc,EAG3F,QAAW,CAAE,QAAS,UAAW,IAAK,UAAW,SAAU,eAAgB,EAC3E,cAAiB,CAAE,QAAS,aAAc,IAAK,gBAAiB,SAAU,eAAgB,EAC1F,WAAc,CAAE,QAAS,UAAW,IAAK,aAAc,SAAU,eAAgB,EACjF,UAAa,CAAE,QAAS,SAAU,IAAK,YAAa,SAAU,eAAgB,EAG9E,MAAS,CAAE,QAAS,eAAgB,IAAK,QAAS,SAAU,aAAc,EAC1E,YAAe,CAAE,QAAS,mBAAoB,IAAK,cAAe,SAAU,UAAW,EAGvF,SAAY,CAAE,QAAS,eAAgB,IAAK,WAAY,SAAU,WAAY,EAC9E,UAAa,CAAE,QAAS,SAAU,IAAK,YAAa,SAAU,WAAY,EAC1E,OAAU,CAAE,QAAS,SAAU,IAAK,YAAa,SAAU,WAAY,EACvE,YAAe,CAAE,QAAS,aAAc,IAAK,cAAe,SAAU,WAAY,EAClF,SAAY,CAAE,QAAS,QAAS,IAAK,WAAY,SAAU,WAAY,CACzE,EAKaC,EAAuB,uCAGvBC,EAAa,GACbC,EAAoB,IACpBC,EAAqB,EACrBC,EAAiB,IC5DvB,SAASC,EAAgBC,EAAkD,CAChF,GAAI,CAACA,EACH,MAAO,CACL,YAAa,GACb,UAAW,EACb,EAIF,OAAW,CAACC,EAASC,CAAI,IAAK,OAAO,QAAQC,CAAW,EACtD,GAAIH,EAAU,SAASC,CAAO,EAC5B,MAAO,CACL,YAAa,GACb,QAASC,EACT,UAAAF,CACF,EAKJ,IAAMI,EAAUJ,EAAU,YAAY,EAGhCK,EAAa,CACjB,aACA,SACA,cACA,eACA,QACF,EAEA,QAAWJ,KAAWI,EACpB,GAAID,EAAQ,SAASH,CAAO,EAC1B,MAAO,CACL,YAAa,GACb,QAAS,CACP,QAAS,UACT,IAAKA,EACL,SAAU,SACZ,EACA,UAAAD,CACF,EAIJ,MAAO,CACL,YAAa,GACb,UAAAA,CACF,CACF,CAKO,SAASM,EAAuBC,EAIpC,CACD,IAAMC,EAAgC,CAAC,EAGjCC,EAAUF,EAAQ,QAAQ,SAAcA,EAAQ,QAAQ,SAC1DE,IACFD,EAAS,QAAU,MAAM,QAAQC,CAAO,EAAIA,EAAQ,CAAC,EAAIA,GAI3D,IAAMC,EAASH,EAAQ,QAAQ,OAC3BG,IACFF,EAAS,OAAS,MAAM,QAAQE,CAAM,EAAIA,EAAO,KAAK,IAAI,EAAIA,GAIhE,IAAMC,EAAWJ,EAAQ,QAAQ,iBAAiB,EAC9CI,IACFH,EAAS,eAAiB,MAAM,QAAQG,CAAQ,EAAIA,EAAS,KAAK,IAAI,EAAIA,GAI5E,IAAMC,EAAWL,EAAQ,QAAQ,iBAAiB,EAClD,OAAIK,IACFJ,EAAS,eAAiB,MAAM,QAAQI,CAAQ,EAAIA,EAAS,CAAC,EAAIA,GAG7DJ,CACT,CC3DO,IAAMK,EAAN,KAAqB,CAM1B,YAAYC,EAAuB,CAJnC,KAAQ,WAA6B,CAAC,EACtC,KAAQ,WAAoC,KAC5C,KAAQ,UAAY,GAGlB,KAAK,OAAS,CACZ,OAAQA,EAAO,OACf,YAAaA,EAAO,aAAeC,EACnC,UAAWD,EAAO,WAAaE,EAC/B,gBAAiBF,EAAO,iBAAmBG,EAC3C,MAAOH,EAAO,OAAS,GACvB,QAASA,EAAO,UAAaI,GAAU,QAAQ,MAAM,oBAAqBA,CAAK,EACjF,CACF,CAKA,MAAM,MAAMC,EAAwC,CAClD,IAAMC,EAA0B,CAC9B,GAAGD,EACH,UAAW,IAAI,KAAK,EAAE,YAAY,CACpC,EAEA,KAAK,WAAW,KAAKC,CAAS,EAE1B,KAAK,OAAO,OACd,QAAQ,IAAI,kCAAmCA,CAAS,EAItD,KAAK,WAAW,QAAU,KAAK,OAAO,UACxC,MAAM,KAAK,MAAM,EAGjB,KAAK,cAAc,CAEvB,CAKA,YACEC,EACAC,EAMAC,EACiC,CACjC,IAAMC,EAAM,IAAI,IAAIF,EAAQ,GAAG,EAE/B,MAAO,CACL,OAAQE,EAAI,SACZ,KAAMA,EAAI,SACV,YAAaH,EAAQ,IACrB,eAAgBA,EAAQ,QACxB,gBAAiBA,EAAQ,SACzB,UAAW,KAAK,eAAeC,EAAQ,QAAQ,YAAY,CAAC,GAAK,GACjE,WAAYA,EAAQ,WACpB,eAAgBA,EAAQ,eACxB,SAAAC,CACF,CACF,CAKQ,eAAgB,CAClB,KAAK,aAET,KAAK,WAAa,WAAW,SAAY,CACvC,KAAK,WAAa,KACd,KAAK,WAAW,OAAS,GAC3B,MAAM,KAAK,MAAM,CAErB,EAAG,KAAK,OAAO,eAAe,EAChC,CAKA,MAAM,OAAQ,CACZ,GAAI,KAAK,WAAa,KAAK,WAAW,SAAW,EAAG,OAEpD,KAAK,UAAY,GACjB,IAAME,EAAS,CAAC,GAAG,KAAK,UAAU,EAClC,KAAK,WAAa,CAAC,EAEnB,GAAI,CACF,MAAM,KAAK,UAAUA,CAAM,CAC7B,OAASP,EAAO,CAEd,KAAK,WAAW,QAAQ,GAAGO,CAAM,EACjC,KAAK,OAAO,QAAQP,CAAc,CACpC,QAAE,CACA,KAAK,UAAY,EACnB,CACF,CAKA,MAAc,UAAUO,EAAwBC,EAAU,EAAkB,CAC1E,GAAI,CACF,IAAMC,EAAW,MAAM,MAAM,KAAK,OAAO,YAAa,CACpD,OAAQ,OACR,QAAS,CACP,eAAgB,mBAChB,cAAiB,UAAU,KAAK,OAAO,MAAM,GAC7C,kBAAmB,OACrB,EACA,KAAM,KAAK,UAAU,CAAE,OAAAF,CAAO,CAAC,CACjC,CAAC,EAED,GAAI,CAACE,EAAS,GACZ,MAAM,IAAI,MAAM,cAAcA,EAAS,MAAM,IAAIA,EAAS,UAAU,EAAE,EAGpE,KAAK,OAAO,OACd,QAAQ,IAAI,0BAA0BF,EAAO,MAAM,SAAS,CAEhE,OAASP,EAAO,CACd,GAAIQ,EAAUE,EAAoB,CAEhC,IAAMC,EAAQC,EAAiB,KAAK,IAAI,EAAGJ,EAAU,CAAC,EACtD,OAAI,KAAK,OAAO,OACd,QAAQ,IAAI,mCAAmCA,CAAO,UAAUG,CAAK,IAAI,EAE3E,MAAM,IAAI,QAAQE,GAAW,WAAWA,EAASF,CAAK,CAAC,EAChD,KAAK,UAAUJ,EAAQC,EAAU,CAAC,CAC3C,CACA,MAAMR,CACR,CACF,CAKQ,eAAec,EAA2D,CAChF,GAAKA,EACL,OAAO,MAAM,QAAQA,CAAM,EAAIA,EAAO,CAAC,EAAIA,CAC7C,CAKA,SAAU,CACJ,KAAK,aACP,aAAa,KAAK,UAAU,EAC5B,KAAK,WAAa,MAGpB,KAAK,MAAM,EAAE,MAAM,IAAM,CAAC,CAAC,CAC7B,CAKA,MAAM,MAA8B,CAClC,GAAI,CACF,IAAMC,EAAe,KAAK,OAAO,YAAY,QAAQ,YAAa,OAAO,EAEnEN,EAAW,MAAM,MAAMM,EAAc,CACzC,OAAQ,MACR,QAAS,CACP,cAAiB,UAAU,KAAK,OAAO,MAAM,GAC7C,kBAAmB,OACrB,CACF,CAAC,EAEKC,EAAO,MAAMP,EAAS,KAAK,EAEjC,OAAKA,EAAS,IAOV,KAAK,OAAO,OACd,QAAQ,IAAI,qCAAsCO,CAAI,EAGjDA,GAVE,CACL,OAAQ,QACR,QAASA,EAAK,SAAW,cAAcP,EAAS,MAAM,EACxD,CAQJ,OAAST,EAAO,CACd,OAAI,KAAK,OAAO,OACd,QAAQ,MAAM,iCAAkCA,CAAK,EAGhD,CACL,OAAQ,QACR,QAASA,aAAiB,MAAQA,EAAM,QAAU,mBACpD,CACF,CACF,CACF,EHrOA,eAAsBiB,EAAKC,EAA8C,CACvE,IAAMC,EAAU,IAAIC,EAAeF,CAAM,EACnCG,EAAS,MAAMF,EAAQ,KAAK,EAClC,OAAAA,EAAQ,QAAQ,EACTE,CACT,CAKO,SAASC,EAAqBJ,EAAuB,CAC1D,IAAMC,EAAU,IAAIC,EAAeF,CAAM,EAEzC,OAAO,SACLK,EACAC,EACAC,EACA,CACA,IAAMC,EAAY,KAAK,IAAI,EAE3B,GAAI,CAEF,IAAMC,EAAYJ,EAAI,QAAQ,YAAY,GAAK,GACzCK,EAAYC,EAAgBF,CAAS,EAE3C,GAAI,CAACC,EAAU,aAAe,CAACA,EAAU,QACvC,OAAOH,EAAK,EAId,IAAMK,EAAcF,EAAU,QAGxBG,EAAcP,EAAI,IACxBA,EAAI,IAAM,YAAkCQ,EAAsE,CAChH,IAAMC,EAAiB,KAAK,IAAI,EAAIP,EAG9BQ,EAAU,GAAGX,EAAI,QAAQ,IAAI,GAAGA,EAAI,GAAG,GACvCY,EAAQhB,EAAQ,YACpBW,EACA,CACE,IAAKI,EACL,QAASX,EAAI,QACb,WAAYC,EAAI,WAChB,eAAAS,CACF,EACAG,EAAuB,CAAE,QAASb,EAAI,OAAe,CAAC,CACxD,EAEA,OAAAJ,EAAQ,MAAMgB,CAAK,EAAE,MAAME,GAAS,CAC9BnB,EAAO,OACT,QAAQ,MAAM,2CAA4CmB,CAAK,CAEnE,CAAC,EAGMN,EAAY,MAAM,KAAMC,CAAI,CACrC,EAEAP,EAAK,CACP,OAASY,EAAO,CACVnB,EAAO,OACT,QAAQ,MAAM,sCAAuCmB,CAAK,EAE5DZ,EAAK,CACP,CACF,CACF,CAKA,eAAsBa,EACpBpB,EACAqB,EAOkB,CAClB,IAAMX,EAAYC,EAAgBU,EAAQ,SAAS,EAEnD,GAAI,CAACX,EAAU,aAAe,CAACA,EAAU,QACvC,MAAO,GAGT,IAAMT,EAAU,IAAIC,EAAeF,CAAM,EAEnCiB,EAAQhB,EAAQ,YACpBS,EAAU,QACV,CACE,IAAKW,EAAQ,IACb,QAAS,CACP,aAAcA,EAAQ,UACtB,GAAIA,EAAQ,SAAW,CAAC,CAC1B,EACA,WAAYA,EAAQ,WACpB,eAAgBA,EAAQ,cAC1B,EACAA,EAAQ,QAAUH,EAAuB,CAAE,QAASG,EAAQ,OAAQ,CAAC,EAAI,MAC3E,EAEA,aAAMpB,EAAQ,MAAMgB,CAAK,EAGzBhB,EAAQ,QAAQ,EAET,EACT","names":["src_exports","__export","AI_CRAWLERS","CrawlerTracker","createNodeMiddleware","detectAICrawler","extractRequestMetadata","ping","trackCrawler","__toCommonJS","AI_CRAWLERS","DEFAULT_API_ENDPOINT","BATCH_SIZE","BATCH_INTERVAL_MS","MAX_RETRY_ATTEMPTS","RETRY_DELAY_MS","detectAICrawler","userAgent","pattern","info","AI_CRAWLERS","lowerUA","aiPatterns","extractRequestMetadata","request","metadata","referer","accept","encoding","language","CrawlerTracker","config","DEFAULT_API_ENDPOINT","BATCH_SIZE","BATCH_INTERVAL_MS","error","event","fullEvent","crawler","request","metadata","url","events","attempt","response","MAX_RETRY_ATTEMPTS","delay","RETRY_DELAY_MS","resolve","header","pingEndpoint","data","ping","config","tracker","CrawlerTracker","result","createNodeMiddleware","req","res","next","startTime","userAgent","detection","detectAICrawler","crawlerInfo","originalEnd","args","responseTimeMs","fullUrl","event","extractRequestMetadata","error","trackCrawler","request"]}